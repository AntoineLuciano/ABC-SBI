# Example: training.yaml - Training configurations with size variants

# Base configuration shared by all variants
base:
  optimizer: "adam"
  weight_decay: 0.0
  validation_split: 0.2
  early_stopping_patience: 10
  store_thetas: true
  num_thetas_to_store: 10000
  # LR scheduler will be loaded separately from lr_schedulers/ directory
  lr_scheduler_name: "cosine"
  lr_scheduler_variant: "default"

# Size variants that override/extend the base configuration
variants:
  # Quick training for testing
  fast:
    learning_rate: 0.001
    num_epochs: 10
    batch_size: 128
    n_samples_per_epoch: 1024
    lr_scheduler_name: "constant"
    lr_scheduler_variant: "default"
    
  # Default balanced training
  default:
    learning_rate: 0.0003
    num_epochs: 100
    batch_size: 256
    n_samples_per_epoch: 10240
    lr_scheduler_name: "cosine"
    lr_scheduler_variant: "default"
    
  # Extended training for better convergence
  extended:
    learning_rate: 0.0001
    num_epochs: 200
    batch_size: 512
    n_samples_per_epoch: 20480
    early_stopping_patience: 20
    lr_scheduler_name: "cosine" 
    lr_scheduler_variant: "with_warmup"
    
  # Heavy training with plateau scheduler
  heavy:
    learning_rate: 0.001
    num_epochs: 500
    batch_size: 512
    n_samples_per_epoch: 51200
    early_stopping_patience: 50
    lr_scheduler_name: "reduce_on_plateau"
    lr_scheduler_variant: "patient"
        
  # Minimal training for debugging
  debug:
    learning_rate: 0.01
    num_epochs: 5
    batch_size: 64
    n_samples_per_epoch: 512
    validation_split: 0.1
    lr_scheduler_name: "constant"
    lr_scheduler_variant: "default"
